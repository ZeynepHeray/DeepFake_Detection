{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1ZviuKV1voHjedpEDMF0Id61pEyd9zn-P",
      "authorship_tag": "ABX9TyNG2KnN/p9+GudRPkFtXoC5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZeynepHeray/DeepFake_Detection/blob/main/deepfake_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#first version of application\nfrom google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "n91zo1pMDe6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02183907-62df-4aea-9eb9-b9aee38698e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install face_recognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt1B2NGpA5cZ",
        "outputId": "3b72770b-b85f-427d-d550-352a6a2634ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting face_recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting face-recognition-models>=0.3.0 (from face_recognition)\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (8.1.7)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (19.24.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_recognition) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from face_recognition) (9.4.0)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566170 sha256=3488bb4ee81f2f31d5a9aae8b78a1768d5f132e12975500a6a217567dceb6a91\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/eb/cf/e9eced74122b679557f597bb7c8e4c739cfcac526db1fd523d\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face_recognition\n",
            "Successfully installed face-recognition-models-0.3.0 face_recognition-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "import face_recognition\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report\n"
      ],
      "metadata": {
        "id": "iHvxghvdnaDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JSON dosyasını yükleme\n",
        "with open('/content/drive/MyDrive/ trainData/metadata.json') as f:\n",
        "    labels = json.load(f)"
      ],
      "metadata": {
        "id": "3kDJjiovuGyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frames_and_labels(video_path, label, frame_interval=30):\n",
        "    video_capture = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    labels = []\n",
        "    frame_count = 0\n",
        "\n",
        "    while video_capture.isOpened():\n",
        "        ret, frame = video_capture.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Belirli aralıklarla frame al\n",
        "        if frame_count % frame_interval == 0:\n",
        "            face_locations = face_recognition.face_locations(frame)\n",
        "            for face_location in face_locations:\n",
        "                top, right, bottom, left = face_location\n",
        "                face_image = frame[top:bottom, left:right]\n",
        "                face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
        "                face_image = Image.fromarray(face_image)\n",
        "                frames.append(face_image)\n",
        "                labels.append(label)\n",
        "        frame_count += 1\n",
        "\n",
        "    video_capture.release()\n",
        "    return frames, labels"
      ],
      "metadata": {
        "id": "UX-Cdzc-uL4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eğitim ve validasyon frameleri ve etiketlerini toplama\n",
        "all_frames = []\n",
        "all_labels = []\n",
        "for video_file, info in labels.items():\n",
        "    label = 0 if info['label'] == 'REAL' else 1\n",
        "    video_path = os.path.join('/content/drive/MyDrive/ trainData/dfdc_train_part_32', video_file)\n",
        "    frames, frame_labels = extract_frames_and_labels(video_path, label)\n",
        "    all_frames.extend(frames)\n",
        "    all_labels.extend(frame_labels)\n",
        "\n",
        "# Veri dönüştürme işlemleri\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Dataset oluşturma\n",
        "class DeepfakeFramesDataset(Dataset):\n",
        "    def __init__(self, frames, labels, transform=None):\n",
        "        self.frames = frames\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.frames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.frames[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Dataset ve DataLoader oluşturma\n",
        "dataset = DeepfakeFramesDataset(all_frames, all_labels, transform=transform)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "pW9Jk7FJuS8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN Modeli tanımlama\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.fc1 = nn.Linear(64*30*30, 128)\n",
        "        self.fc2 = nn.Linear(128, 2)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64*30*30)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CNN()"
      ],
      "metadata": {
        "id": "9Inbcgx2ufVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kayıp fonksiyonu ve optimizasyon\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "P_F2wH_Hujk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model eğitimi\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "    # Validasyon\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "    print(f'Validation Loss: {val_loss/len(val_loader)}')"
      ],
      "metadata": {
        "id": "fuCVyD2DurJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb06e556-7dbb-4c97-b47b-d9c404086ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.5506237546602885\n",
            "Validation Loss: 0.5122129457692305\n",
            "Epoch 2, Loss: 0.5223267258948119\n",
            "Validation Loss: 0.5161361924062172\n",
            "Epoch 3, Loss: 0.5158992468049286\n",
            "Validation Loss: 0.5134463955958685\n",
            "Epoch 4, Loss: 0.4622532058329809\n",
            "Validation Loss: 0.43148843695720035\n",
            "Epoch 5, Loss: 0.39898122374027495\n",
            "Validation Loss: 0.3620518756409486\n",
            "Epoch 6, Loss: 0.330874224168756\n",
            "Validation Loss: 0.3835407417888443\n",
            "Epoch 7, Loss: 0.29505241760816525\n",
            "Validation Loss: 0.333775845511506\n",
            "Epoch 8, Loss: 0.24663479241823394\n",
            "Validation Loss: 0.3084087944589555\n",
            "Epoch 9, Loss: 0.1966148923648886\n",
            "Validation Loss: 0.3226826707832515\n",
            "Epoch 10, Loss: 0.16003059434196937\n",
            "Validation Loss: 0.32218177942559123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test verilerini framelere ayırma ve etiketlerini alma\n",
        "def extract_test_frames_and_labels(json_path, video_dir, frame_interval=30):\n",
        "    with open(json_path) as f:\n",
        "        test_labels = json.load(f)\n",
        "\n",
        "    all_test_frames = []\n",
        "    all_test_labels = []\n",
        "    for video_file, info in test_labels.items():\n",
        "        label = 0 if info['label'] == 'REAL' else 1\n",
        "        video_path = os.path.join(video_dir, video_file)\n",
        "        frames, frame_labels = extract_frames_and_labels(video_path, label, frame_interval)\n",
        "        all_test_frames.extend(frames)\n",
        "        all_test_labels.extend(frame_labels)\n",
        "    return all_test_frames, all_test_labels\n",
        "# Test frame extraction\n",
        "test_frames, test_labels = extract_test_frames_and_labels('/content/drive/MyDrive/testData/metadata.json', '/content/drive/MyDrive/testData/data')\n",
        "\n",
        "# Test dataset ve DataLoader oluşturma\n",
        "test_dataset = DeepfakeFramesDataset(test_frames, test_labels, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Test verileriyle modelin performansını değerlendirme\n",
        "def classify_test_data(test_loader, model):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    return all_predictions, all_labels\n",
        "\n",
        "predicted_labels, true_labels = classify_test_data(test_loader, model)\n",
        "\n",
        "# Performans değerlendirme\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "report = classification_report(true_labels, predicted_labels)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Classification Report:\\n {report}')\n"
      ],
      "metadata": {
        "id": "k44Rfyv7uyF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c130b007-f8fb-4bef-9d4b-906da80f74e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6371514694800301\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.14      0.21      0.17       912\n",
            "           1       0.82      0.73      0.77      4396\n",
            "\n",
            "    accuracy                           0.64      5308\n",
            "   macro avg       0.48      0.47      0.47      5308\n",
            "weighted avg       0.70      0.64      0.66      5308\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_frames = []\n",
        "sample_labels = []\n",
        "count = 0\n",
        "frame_interval = 30\n",
        "\n",
        "with open('/content/drive/MyDrive/testData/metadata.json') as f:\n",
        "    test_labels = json.load(f)\n",
        "\n",
        "for video_file, info in test_labels.items():\n",
        "    if count >= 10:\n",
        "        break\n",
        "    label = 0 if info['label'] == 'REAL' else 1\n",
        "    video_path = os.path.join('/content/drive/MyDrive/testData/data', video_file)\n",
        "    video_capture = cv2.VideoCapture(video_path)\n",
        "    frame_count = 0\n",
        "    while video_capture.isOpened():\n",
        "        ret, frame = video_capture.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_count % frame_interval == 0:\n",
        "            face_locations = face_recognition.face_locations(frame)\n",
        "            if face_locations:\n",
        "                top, right, bottom, left = face_locations[0]\n",
        "                face_image = frame[top:bottom, left:right]\n",
        "                face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
        "                face_image = Image.fromarray(face_image)\n",
        "                sample_frames.append(face_image)\n",
        "                sample_labels.append(label)\n",
        "                count += 1\n",
        "                break\n",
        "        frame_count += 1\n",
        "    video_capture.release()\n",
        "\n",
        "sample_transformed_frames = [transform(frame).unsqueeze(0) for frame in sample_frames]\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, frame in enumerate(sample_transformed_frames):\n",
        "        output = model(frame)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        print(f'Video {i+1}:')\n",
        "        print(f'Gerçek Etiket: {\"REAL\" if sample_labels[i] == 0 else \"FAKE\"}')\n",
        "        print(f'Tahmin Edilen Etiket: {\"REAL\" if predicted.item() == 0 else \"FAKE\"}\\n')\n"
      ],
      "metadata": {
        "id": "UM6ro3SVu8_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89c4e290-5bc3-4ebf-e9f8-799c5e98a3f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video 1:\n",
            "Gerçek Etiket: FAKE\n",
            "Tahmin Edilen Etiket: FAKE\n",
            "\n",
            "Video 2:\n",
            "Gerçek Etiket: REAL\n",
            "Tahmin Edilen Etiket: REAL\n",
            "\n",
            "Video 3:\n",
            "Gerçek Etiket: REAL\n",
            "Tahmin Edilen Etiket: FAKE\n",
            "\n",
            "Video 4:\n",
            "Gerçek Etiket: FAKE\n",
            "Tahmin Edilen Etiket: REAL\n",
            "\n",
            "Video 5:\n",
            "Gerçek Etiket: FAKE\n",
            "Tahmin Edilen Etiket: REAL\n",
            "\n",
            "Video 6:\n",
            "Gerçek Etiket: FAKE\n",
            "Tahmin Edilen Etiket: FAKE\n",
            "\n",
            "Video 7:\n",
            "Gerçek Etiket: FAKE\n",
            "Tahmin Edilen Etiket: FAKE\n",
            "\n",
            "Video 8:\n",
            "Gerçek Etiket: FAKE\n",
            "Tahmin Edilen Etiket: REAL\n",
            "\n",
            "Video 9:\n",
            "Gerçek Etiket: FAKE\n",
            "Tahmin Edilen Etiket: FAKE\n",
            "\n",
            "Video 10:\n",
            "Gerçek Etiket: FAKE\n",
            "Tahmin Edilen Etiket: FAKE\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DeeperCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeeperCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, 1)\n",
        "        self.fc1 = nn.Linear(256*15*15, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 2)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = self.pool(self.relu(self.conv4(x)))\n",
        "        x = x.view(-1, 256*15*15)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model2 = DeeperCNN()"
      ],
      "metadata": {
        "id": "76ezP7XEjfP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Model eğitimi\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model2.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "    # Validasyon\n",
        "    model2.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "    print(f'Validation Loss: {val_loss/len(val_loader)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBlyy1XmkDfh",
        "outputId": "ad8fb912-5323-42cb-a944-887c94e7d606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.11825048334147564\n",
            "Validation Loss: 0.34932170777271193\n",
            "Epoch 2, Loss: 0.09792607967438738\n",
            "Validation Loss: 0.3714068247160564\n",
            "Epoch 3, Loss: 0.09705354839504238\n",
            "Validation Loss: 0.46602787244288874\n",
            "Epoch 4, Loss: 0.08054328157453153\n",
            "Validation Loss: 0.4120370297071834\n",
            "Epoch 5, Loss: 0.07186811430276269\n",
            "Validation Loss: 0.4599178630160168\n",
            "Epoch 6, Loss: 0.06514994791713814\n",
            "Validation Loss: 0.42928964966752875\n",
            "Epoch 7, Loss: 0.05923913361593371\n",
            "Validation Loss: 0.47269613776976865\n",
            "Epoch 8, Loss: 0.051136774105197264\n",
            "Validation Loss: 0.4768809984670952\n",
            "Epoch 9, Loss: 0.046644350541995776\n",
            "Validation Loss: 0.5004799435070405\n",
            "Epoch 10, Loss: 0.0463238449283092\n",
            "Validation Loss: 0.6808922912556833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_test_data(test_loader, model):\n",
        "    model2.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    return all_predictions, all_labels\n",
        "\n",
        "predicted_labels, true_labels = classify_test_data(test_loader, model)\n",
        "\n",
        "# Performans değerlendirme\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "report = classification_report(true_labels, predicted_labels)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Classification Report:\\n {report}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN_JJmY6kdmv",
        "outputId": "37d42a49-5b42-42da-8b5d-ab343d14600a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6554257724189902\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.15      0.22      0.18       912\n",
            "           1       0.82      0.75      0.78      4396\n",
            "\n",
            "    accuracy                           0.66      5308\n",
            "   macro avg       0.49      0.48      0.48      5308\n",
            "weighted avg       0.71      0.66      0.68      5308\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
